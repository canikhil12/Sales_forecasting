{"cells":[{"cell_type":"code","source":["!apt-get -qq update && apt-get -qq install git\n","!git config --global user.name  \"canikhil12\"\n","!git config --global user.email \"canikhil3@gmail.com\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ze6hKX3xvqKD","executionInfo":{"status":"ok","timestamp":1751470169869,"user_tz":240,"elapsed":139659,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"db4e6a61-12e2-4262-d3c4-2db628efc8f9"},"id":"Ze6hKX3xvqKD","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"]}]},{"cell_type":"code","source":["Token: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"],"metadata":{"id":"5JqNjUN0waI3"},"id":"5JqNjUN0waI3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from urllib.parse import quote\n","import getpass\n","\n","token =\"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\"\n","username = 'canikhil12'\n","repo  = \"Sales_forecasting\"\n","# Create authenticated HTTPS URL with token\n","url = f\"https://{token}@github.com/{username}/{repo}.git\"\n","print (remote_url)\n","# Set the remote to use the new URL\n","!git remote set-url origin {remote_url}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7e2SrpBsw1sD","executionInfo":{"status":"ok","timestamp":1751473831604,"user_tz":240,"elapsed":118,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"f4f02ca5-1994-4f1c-98ee-5c492c54cad2"},"id":"7e2SrpBsw1sD","execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["https://Akhil%40134@github.com/canikhil12/Sales_forecasting.git\n","error: No such remote 'origin'\n"]}]},{"cell_type":"code","source":["\n","!cp \"/content/drive/MyDrive/Colab Notebooks/databricks_inventory_forecast_xgboost.ipynb\" \\\n","     notebooks/databricks_inventory_forecast_xgboost.ipynb\n","!git add notebooks/databricks_inventory_forecast_xgboost.ipynb\n","!git commit -m \"Initial commit ‚Äì Colab notebook\"\n","!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaalBifAx6-i","executionInfo":{"status":"ok","timestamp":1751474053521,"user_tz":240,"elapsed":1024,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"97200d45-0d48-4aa3-d692-0010ab385ae3"},"id":"kaalBifAx6-i","execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mSales_forecasting/\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n","fatal: could not read Password for 'https://Akhil%40134@github.com': No such device or address\n"]}]},{"cell_type":"code","source":["%cd /content/Sales_forecasting    # or your repo folder\n","\n","# Make your edits (e.g., add notebooks or Python files)\n","!git add .\n","!git commit -m \"Update from Colab\"\n","!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-JaDq_j7D8j","executionInfo":{"status":"ok","timestamp":1751473026505,"user_tz":240,"elapsed":923,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"f5aa3ac9-9497-4875-b3c4-ff13bcd82d03"},"id":"G-JaDq_j7D8j","execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/Sales_forecasting # or your repo folder'\n","/content/Sales_forecasting/Sales_forecasting/Sales_forecasting\n","On branch main\n","Your branch is ahead of 'origin/main' by 1 commit.\n","  (use \"git push\" to publish your local commits)\n","\n","nothing to commit, working tree clean\n","remote: Support for password authentication was removed on August 13, 2021.\n","remote: Please see https://docs.github.com/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\n","fatal: Authentication failed for 'https://github.com/canikhil12/Sales_forecasting/'\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"PoIr9nO-7DZN"},"id":"PoIr9nO-7DZN"},{"cell_type":"code","execution_count":6,"id":"585ca00f","metadata":{"id":"585ca00f","executionInfo":{"status":"ok","timestamp":1751470652255,"user_tz":240,"elapsed":1201,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}}},"outputs":[],"source":["# üîê Shopify API Configuration\n","import requests\n","import csv\n","import urllib3\n","import pandas as pd\n","import os\n","from datetime import datetime\n","from difflib import get_close_matches\n","from sqlalchemy import create_engine\n","\n","urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n","\n","# CONFIG\n","SHOP_NAME = \"perfumora\"\n","ACCESS_TOKEN = \"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\"\n","API_VERSION = \"2024-04\"\n","BASE_URL = f\"https://{SHOP_NAME}.myshopify.com/admin/api/{API_VERSION}/orders.json\"\n","HEADERS = {\n","    \"X-Shopify-Access-Token\": ACCESS_TOKEN,\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# Column mapping rules\n","EXPECTED_COLUMNS = {\n","    'sku': ['sku', 'SKU', 'product_code', 'item_id'],\n","    'product_name': ['product_name', 'Product Name', 'name'],\n","    'cost_price': ['cost', 'price', 'cost_price'],\n","    'available_stock': ['stock', 'inventory', 'available_stock'],\n","    'popularity_score': ['popularity_score', 'trend_score'],\n","    'avg_market_price': ['avg_market_price', 'market_price'],\n","    'quantity': ['quantity', 'qty', 'units'],\n","    'price': ['price', 'unit_price'],\n","    'total': ['total', 'total_price'],\n","    'created_at': ['created_at', 'order_date', 'date']\n","}\n","# FUNCTIONS"]},{"cell_type":"code","execution_count":null,"id":"188361bb","metadata":{"id":"188361bb"},"outputs":[],"source":["# üß† Clean Column Names\n","def clean_column(df):\n","    rename_map = {}\n","    for expected, variants in EXPECTED_COLUMNS.items():\n","        found = next((col for col in df.columns if col.strip().lower() in [v.lower() for v in variants]), None)\n","        if not found:\n","            close_matches = get_close_matches(expected, df.columns, n=1, cutoff=0.7)\n","            if close_matches:\n","                found = close_matches[0]\n","        if found:\n","            rename_map[found] = expected\n","    df = df.rename(columns=rename_map)\n","    missing = [col for col in ['sku', 'product_name'] if col not in df.columns]\n","    if missing:\n","        raise ValueError(f\"Missing required columns: {missing}\")\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"68f732a3","metadata":{"id":"68f732a3"},"outputs":[],"source":["import pandas as pd\n","import os\n","from sqlalchemy import create_engine\n","\n","def generate_restock_report(sales_csv_path, supplier_folder_path):\n","    # Load sales data\n","    sales_df = pd.read_csv(sales_csv_path, parse_dates=[\"created_at\"])\n","    sales_df[\"date\"] = sales_df[\"created_at\"].dt.date\n","    sales_df[\"quantity\"] = pd.to_numeric(sales_df[\"quantity\"], errors=\"coerce\").fillna(0)\n","\n","    # Load supplier CSVs\n","    supplier_files = [f for f in os.listdir(supplier_folder_path) if f.endswith(\".csv\")]\n","    supplier_df = pd.concat([\n","        pd.read_csv(os.path.join(supplier_folder_path, f))\n","        for f in supplier_files\n","    ], ignore_index=True)\n","\n","    supplier_df[\"available_stock\"] = pd.to_numeric(supplier_df[\"available_stock\"], errors=\"coerce\").fillna(0)\n","\n","    # Summarize sales: avg daily quantity per SKU\n","    summary = (\n","        sales_df.groupby([\"sku\", \"date\"])[\"quantity\"].sum()\n","        .groupby(\"sku\")\n","        .agg(avg_daily_sales=\"mean\", days_sold=\"count\")\n","        .reset_index()\n","    )\n","\n","    # Merge and calculate days of coverage\n","    merged_df = pd.merge(supplier_df, summary, on=\"sku\", how=\"left\").fillna({\"avg_daily_sales\": 0})\n","    merged_df[\"days_coverage\"] = merged_df[\"available_stock\"] / merged_df[\"avg_daily_sales\"].replace(0, 0.01)\n","    merged_df[\"restock_needed\"] = merged_df[\"days_coverage\"] < 15\n","\n","    # Save to PostgreSQL (optional)\n","    conn_string = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","    engine = create_engine(conn_string)\n","    merged_df.to_sql(\"restock_recommendations\", engine, if_exists=\"replace\", index=False)\n","\n","    print(\"‚úÖ Restocking report generated and saved to PostgreSQL as 'restock_recommendations'\")\n"]},{"cell_type":"code","execution_count":null,"id":"e27069c9","metadata":{"id":"e27069c9"},"outputs":[],"source":["def generate_sourcing_recommendation(trend_path, supplier_dir, db_uri, table_name=\"sourcing_recommendations\"):\n","    # Load trend product file (CSV)\n","    trend_df = pd.read_csv(trend_path)\n","\n","    # Combine all supplier CSVs into one DataFrame\n","    supplier_files = [f for f in os.listdir(supplier_dir) if f.endswith(\".csv\")]\n","    supplier_df = pd.concat([\n","        pd.read_csv(os.path.join(supplier_dir, f)) for f in supplier_files\n","    ], ignore_index=True)\n","\n","    # Merge by SKU\n","    merged_df = pd.merge(trend_df, supplier_df, on=\"sku\", how=\"inner\")\n","\n","    # Calculate potential margin\n","    merged_df[\"potential_margin\"] = merged_df[\"avg_market_price\"] - merged_df[\"cost_price\"]\n","\n","    # Filter for good sourcing opportunities\n","    filtered = merged_df[\n","        (merged_df[\"potential_margin\"] > 10) &\n","        (merged_df[\"popularity_score\"] >= 0.75)\n","    ].sort_values(by=[\"potential_margin\", \"popularity_score\"], ascending=False)\n","\n","    # Save to PostgreSQL\n","    engine = create_engine(db_uri)\n","    filtered.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n","\n","    print(f\"‚úÖ Sourcing recommendations saved to PostgreSQL table: {table_name}\")"]},{"cell_type":"code","execution_count":null,"id":"2ac44022","metadata":{"id":"2ac44022"},"outputs":[],"source":["# üßæ Combine into Final Inventory Actions\n","def (restock_path, source_path, output_path):\n","    restock_df = spark.read.option(\"header\", \"true\").csv(restock_path)\n","    sourcing_df = spark.read.option(\"header\", \"true\").csv(source_path)\n","\n","    restock_df['action'] = 'Restock'\n","    sourcing_df['action'] = 'Source New'\n","\n","    restock_df = restock_df.rename(columns={'available_stock': 'stock_available', 'avg_daily_sales': 'daily_sales_rate'})\n","    sourcing_df = sourcing_df.rename(columns={'product_name_x': 'product_name', 'available_stock': 'stock_available'})\n","\n","    restock_cols = ['sku', 'product_name', 'stock_available', 'daily_sales_rate', 'days_coverage', 'action']\n","    sourcing_cols = ['sku', 'product_name', 'stock_available', 'popularity_score', 'potential_margin', 'action']\n","\n","    combined_df = pd.concat([restock_df[restock_cols], sourcing_df[sourcing_cols]], ignore_index=True)\n","    combined_df.write.format('delta').mode('overwrite').save('/mnt/delta/final_inventory_actions')\n","spark.sql(\"CREATE TABLE IF NOT EXISTS final_inventory_actions USING DELTA LOCATION '/mnt/delta/final_inventory_actions'\")\n","    print(f\"‚úÖ Combined CSV saved as: {output_path}\")\n","\n","# MAIN EXECUTION"]},{"cell_type":"code","execution_count":null,"id":"7de476f0","metadata":{"id":"7de476f0"},"outputs":[],"source":["# üîÅ Run the Full Pipeline\n","if __name__ == '__main__':\n","    orders = fetch_orders(\"2024-01-01\", \"2024-01-15\")\n","    save_orders_to_csv(orders, \"shopify_orders.csv\")\n","\n","    #generate_restock_report(\"shopify_orders.csv\", \"supplier_lists\", \"restock_recommendations.csv\")\n","    #generate_sourcing_recommendation(\"trending_products.csv\", \"supplier_lists\", \"sourcing_recommendations.csv\")\n","    #generate_final_inventory_actions(\"restock_recommendations.csv\", \"sourcing_recommendations.csv\", \"final_inventory_actions.csv\")"]},{"cell_type":"code","execution_count":null,"id":"f1a456c9","metadata":{"id":"f1a456c9"},"outputs":[],"source":["\n","# üì¶ LightGBM Sales Forecasting Block\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMRegressor\n","from sklearn.metrics import mean_absolute_error\n","import joblib\n","import os\n","\n","# Load combined dataset\n","df = pd.read_csv(\"/dbfs/mnt/perfumora/data/combined_features.csv\")\n","\n","# Extract date-related features\n","df['created_at'] = pd.to_datetime(df['created_at'])\n","df['day_of_week'] = df['created_at'].dt.dayofweek\n","df['day_of_month'] = df['created_at'].dt.day\n","\n","# Fill missing values\n","df['available_stock'] = df['available_stock'].fillna(0)\n","df['popularity_score'] = df['popularity_score'].fillna(0)\n","\n","# Aggregate sales by SKU and date\n","sales_agg = df.groupby(['sku', 'created_at']).agg({\n","    'quantity': 'sum',\n","    'available_stock': 'mean',\n","    'popularity_score': 'mean',\n","    'avg_market_price': 'mean'\n","}).reset_index()\n","\n","# Create 7-day rolling future target\n","sales_agg['target_sales'] = (\n","    sales_agg.groupby('sku')['quantity']\n","    .shift(-1)\n","    .rolling(window=7)\n","    .sum()\n","    .reset_index(level=0, drop=True)\n",")\n","\n","sales_agg = sales_agg.dropna(subset=['target_sales'])\n","\n","# Re-add time-based features\n","sales_agg['day_of_week'] = sales_agg['created_at'].dt.dayofweek\n","sales_agg['day_of_month'] = sales_agg['created_at'].dt.day\n","\n","# Define features and target\n","features = ['available_stock', 'popularity_score', 'avg_market_price', 'day_of_week', 'day_of_month']\n","X = sales_agg[features]\n","y = sales_agg['target_sales']\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n","\n","# Train model\n","model = LGBMRegressor()\n","model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = model.predict(X_test)\n","mae = mean_absolute_error(y_test, y_pred)\n","print(f\"‚úÖ MAE on test set: {mae:.2f}\")\n","\n","# Save model\n","model_path = \"/dbfs/mnt/perfumora/models/lgbm_sales_forecast.pkl\"\n","os.makedirs(os.path.dirname(model_path), exist_ok=True)\n","joblib.dump(model, model_path)\n","\n","# Output predictions\n","sample_output = X_test.copy()\n","sample_output['actual_sales'] = y_test\n","sample_output['predicted_sales'] = y_pred\n","sample_output.to_csv(\"/dbfs/mnt/perfumora/output/sample_forecast.csv\", index=False)\n","print(\"üìä Sample forecast output saved.\")\n"]},{"cell_type":"code","execution_count":null,"id":"54647bef","metadata":{"id":"54647bef"},"outputs":[],"source":["\n","# üìà XGBoost-based Daily Sales Forecast (30 Days) per SKU\n","import pandas as pd\n","import numpy as np\n","from xgboost import XGBRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","\n","# Read historical Shopify orders\n","sales_df = spark.read.option(\"header\", \"true\").csv(\"/mnt/perfumora/orders/shopify_orders.csv\").toPandas()\n","sales_df[\"created_at\"] = pd.to_datetime(sales_df[\"created_at\"])\n","sales_df[\"date\"] = sales_df[\"created_at\"].dt.date\n","\n","# Aggregate quantity per sku per day\n","grouped = (\n","    sales_df.groupby([\"sku\", \"date\"])[\"quantity\"]\n","    .sum()\n","    .reset_index()\n","    .rename(columns={\"date\": \"ds\", \"quantity\": \"y\"})\n",")\n","\n","forecast_dfs = []\n","unique_skus = grouped[\"sku\"].unique()\n","\n","for sku in unique_skus:\n","    sku_df = grouped[grouped[\"sku\"] == sku].copy()\n","    if len(sku_df) < 30:\n","        continue\n","\n","    sku_df[\"ds\"] = pd.to_datetime(sku_df[\"ds\"])\n","    sku_df = sku_df.sort_values(\"ds\")\n","    sku_df[\"dayofweek\"] = sku_df[\"ds\"].dt.dayofweek\n","    sku_df[\"day\"] = sku_df[\"ds\"].dt.day\n","    sku_df[\"month\"] = sku_df[\"ds\"].dt.month\n","    sku_df[\"lag_1\"] = sku_df[\"y\"].shift(1)\n","    sku_df[\"lag_7\"] = sku_df[\"y\"].shift(7)\n","    sku_df = sku_df.dropna()\n","\n","    X = sku_df[[\"dayofweek\", \"day\", \"month\", \"lag_1\", \"lag_7\"]]\n","    y = sku_df[\"y\"]\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","\n","    model = XGBRegressor(n_estimators=100, learning_rate=0.1)\n","    model.fit(X_train, y_train)\n","\n","    future_dates = pd.date_range(sku_df[\"ds\"].max(), periods=31, freq='D')[1:]\n","    future_df = pd.DataFrame({\"ds\": future_dates})\n","    future_df[\"dayofweek\"] = future_df[\"ds\"].dt.dayofweek\n","    future_df[\"day\"] = future_df[\"ds\"].dt.day\n","    future_df[\"month\"] = future_df[\"ds\"].dt.month\n","    # Use latest known y for lag_1 and lag_7\n","    future_df[\"lag_1\"] = y.values[-1]\n","    future_df[\"lag_7\"] = y.values[-7] if len(y) >= 7 else y.values[-1]\n","\n","    future_df[\"yhat\"] = model.predict(future_df[[\"dayofweek\", \"day\", \"month\", \"lag_1\", \"lag_7\"]])\n","    future_df[\"sku\"] = sku\n","    forecast_dfs.append(future_df[[\"ds\", \"yhat\", \"sku\"]])\n","\n","if forecast_dfs:\n","    result_df = pd.concat(forecast_dfs)\n","    forecast_spark_df = spark.createDataFrame(result_df)\n","    forecast_spark_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/perfumora/delta/sku_forecast_xgb\")\n","    spark.sql(\"DROP TABLE IF EXISTS perfumora.sku_forecast_xgb\")\n","    spark.sql(\"CREATE TABLE perfumora.sku_forecast_xgb USING DELTA LOCATION '/mnt/perfumora/delta/sku_forecast_xgb'\")\n","    print(\"‚úÖ XGBoost forecast saved as Delta table: perfumora.sku_forecast_xgb\")\n","else:\n","    print(\"‚ö†Ô∏è No forecasts were generated.\")\n"]},{"cell_type":"code","source":["import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# Your Neon PostgreSQL connection string\n","conn_string = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","\n","# Create SQLAlchemy engine\n","engine = create_engine(conn_string)\n","\n","# Test query\n","try:\n","    df = pd.read_sql(\"SELECT NOW()\", con=engine)\n","    print(\"‚úÖ Connected to Neon PostgreSQL!\")\n","    print(df)\n","except Exception as e:\n","    print(\"‚ùå Connection failed:\", e)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2CYw2evL9ea5","executionInfo":{"status":"ok","timestamp":1749242212755,"user_tz":240,"elapsed":815,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"bdb6e7ed-64ae-41e6-81a1-fd9393c7bc19"},"id":"2CYw2evL9ea5","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Connected to Neon PostgreSQL!\n","                               now\n","0 2025-06-06 20:36:52.453779+00:00\n"]}]},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# Shopify API config\n","SHOP_NAME = \"perfumora\"\n","ACCESS_TOKEN = \"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\"\n","API_VERSION = \"2024-04\"\n","BASE_URL = f\"https://{SHOP_NAME}.myshopify.com/admin/api/{API_VERSION}/orders.json\"\n","HEADERS = {\n","    \"X-Shopify-Access-Token\": ACCESS_TOKEN,\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# üîÑ Fetch Orders from Shopify\n","def fetch_orders(from_date, to_date):\n","    print(f\"üì¶ Fetching orders from {from_date} to {to_date}...\")\n","    orders = []\n","    url = f\"{BASE_URL}?status=any&created_at_min={from_date}&created_at_max={to_date}&limit=250\"\n","    while url:\n","        response = requests.get(url, headers=HEADERS)\n","        if response.status_code != 200:\n","            print(f\"‚ùå Error: {response.status_code}\")\n","            break\n","        data = response.json()\n","        batch = data.get(\"orders\", [])\n","        print(f\"‚úÖ Got {len(batch)} orders\")\n","        orders.extend(batch)\n","\n","        # Handle pagination\n","        link = response.headers.get(\"Link\", \"\")\n","        next_url = None\n","        for part in link.split(\",\"):\n","            if 'rel=\"next\"' in part:\n","                next_url = part[part.find(\"<\") + 1:part.find(\">\")]\n","        url = next_url\n","    return orders\n","\n","# üíæ Save Orders to PostgreSQL\n","def save_orders_to_postgresql(orders):\n","    rows = []\n","    for order in orders:\n","        customer = order.get(\"customer\", {})\n","        customer_name = f\"{customer.get('first_name', '')} {customer.get('last_name', '')}\".strip()\n","        for item in order.get(\"line_items\", []):\n","            rows.append({\n","                \"order_id\": order.get(\"id\"),\n","                \"created_at\": order.get(\"created_at\"),\n","                \"customer_name\": customer_name,\n","                \"product\": item.get(\"name\"),\n","                \"sku\": item.get(\"sku\", \"\"),\n","                \"quantity\": item.get(\"quantity\"),\n","                \"price\": item.get(\"price\"),\n","                \"total\": order.get(\"total_price\")\n","            })\n","\n","    df = pd.DataFrame(rows)\n","    conn_string = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","    engine = create_engine(conn_string)\n","    df.to_sql(\"orders\", engine, if_exists=\"append\", index=False)\n","    print(f\"‚úÖ {len(df)} orders saved to PostgreSQL\")\n","\n","# üîÅ Run the Full Pipeline\n","if __name__ == \"__main__\":\n","    orders = fetch_orders(\"2024-01-01\", \"2024-01-15\")\n","    print(f\"üìä Total orders fetched: {len(orders)}\")\n","    if orders:\n","        save_orders_to_postgresql(orders)\n","    else:\n","        print(\"‚ö†Ô∏è No orders to save.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5m3jwyZk_rXn","executionInfo":{"status":"ok","timestamp":1749242799465,"user_tz":240,"elapsed":9494,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"9740c033-89e7-4d55-8bcc-189ed62ded66"},"id":"5m3jwyZk_rXn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üì¶ Fetching orders from 2024-01-01 to 2024-01-15...\n","‚úÖ Got 250 orders\n","‚úÖ Got 130 orders\n","üìä Total orders fetched: 380\n","‚úÖ 385 orders saved to PostgreSQL\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# Neon PostgreSQL connection\n","conn_str = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","engine = create_engine(conn_str)\n","\n","# Folder paths\n","supplier_folder = '/content/supplier_lists'\n","my_stock_folder = '/content/my_stock'\n","trend_data_folder='/content/trend_data'\n","\n","# --- Step 1: Upload supplier data ---\n","supplier_files = [f for f in os.listdir(supplier_folder) if f.endswith(\".csv\")]\n","supplier_data = []\n","\n","for file in supplier_files:\n","    file_path = os.path.join(supplier_folder, file)\n","    df = pd.read_csv(file_path)\n","    supplier_name = os.path.splitext(file)[0]\n","    df[\"supplier_name\"] = supplier_name\n","    df[\"source\"] = \"supplier\"\n","    supplier_data.append(df)\n","\n","supplier_df = pd.concat(supplier_data, ignore_index=True)\n","supplier_df.to_sql(\"supplier_master\", engine, if_exists=\"replace\", index=False)\n","print(\"‚úÖ Uploaded supplier data to 'supplier_master' table.\")\n","\n","# --- Step 2: Upload my_stock data ---\n","stock_files = [f for f in os.listdir(my_stock_folder) if f.endswith(\".csv\")]\n","stock_data = []\n","\n","for file in stock_files:\n","    file_path = os.path.join(my_stock_folder, file)\n","    df = pd.read_csv(file_path)\n","    stock_source = os.path.splitext(file)[0]\n","    df[\"stock_source\"] = stock_source\n","    df[\"source\"] = \"my_stock\"\n","    stock_data.append(df)\n","\n","stock_df = pd.concat(stock_data, ignore_index=True)\n","stock_df.to_sql(\"my_stock\", engine, if_exists=\"replace\", index=False)\n","print(\"‚úÖ Uploaded stock data to 'my_stock' table.\")\n","\n","trend_files = [f for f in os.listdir(trend_data_folder) if f.endswith(\".csv\")]\n","trend_data = []\n","for file in trend_files:\n","    file_path = os.path.join(trend_data_folder, file)\n","    df = pd.read_csv(file_path)\n","    trend_source = os.path.splitext(file)[0]\n","    df[\"trend_source\"] = trend_source\n","    df[\"source\"] = \"trend_data\"\n","    stock_data.append(df)\n","\n","stock_df = pd.concat(stock_data, ignore_index=True)\n","stock_df.to_sql(\"trend_data\", engine, if_exists=\"replace\", index=False)\n","print(\"‚úÖ Uploaded trend data to 'trend_data' table.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXXfo2xsBldH","executionInfo":{"status":"ok","timestamp":1749329846070,"user_tz":240,"elapsed":4102,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"7317f7a4-0dd2-421a-d923-04478540d048"},"id":"EXXfo2xsBldH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Uploaded supplier data to 'supplier_master' table.\n","‚úÖ Uploaded stock data to 'my_stock' table.\n","‚úÖ Uploaded trend data to 'trend_data' table.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"auukG3L9Cxjc"},"id":"auukG3L9Cxjc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sqlalchemy import create_engine\n","\n","def generate_restock_report_from_neon():\n","    # PostgreSQL connection\n","    conn_str = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","    engine = create_engine(conn_str)\n","\n","    # üì• Load order data (sales)\n","    orders_df = pd.read_sql(\"SELECT sku, quantity, created_at FROM orders\", engine)\n","    orders_df[\"created_at\"] = pd.to_datetime(orders_df[\"created_at\"])\n","    orders_df[\"date\"] = orders_df[\"created_at\"].dt.date\n","    orders_df[\"quantity\"] = pd.to_numeric(orders_df[\"quantity\"], errors=\"coerce\").fillna(0)\n","\n","    # üì• Load my_stock and rename correctly\n","    my_stock_df = pd.read_sql(\"SELECT * FROM my_stock\", engine)\n","    my_stock_df[\"my_stock_qty\"] = pd.to_numeric(my_stock_df[\"Quantity On Hand\"], errors=\"coerce\").fillna(0)\n","\n","    # üì• Load supplier_master and rename correctly\n","    supplier_df = pd.read_sql(\"SELECT * FROM supplier_master\", engine)\n","    supplier_df[\"supplier_stock_qty\"] = pd.to_numeric(supplier_df[\"available_stock\"], errors=\"coerce\").fillna(0)\n","\n","    # üìä Calculate avg daily sales per SKU\n","    summary = (\n","        orders_df.groupby([\"sku\", \"date\"])[\"quantity\"].sum()\n","        .groupby(\"sku\")\n","        .agg(avg_daily_sales=\"mean\", days_sold=\"count\")\n","        .reset_index()\n","    )\n","\n","    # üîÅ Merge sales summary with my_stock\n","    stock_summary = pd.merge(my_stock_df, summary, on=\"sku\", how=\"left\").fillna({\"avg_daily_sales\": 0})\n","    stock_summary[\"days_coverage\"] = stock_summary[\"my_stock_qty\"] / stock_summary[\"avg_daily_sales\"].replace(0, 0.01)\n","    stock_summary[\"restock_needed\"] = stock_summary[\"days_coverage\"] < 15\n","\n","    # üîÅ Join only restock-needed items with supplier data\n","    restock_df = pd.merge(stock_summary[stock_summary[\"restock_needed\"] == True], supplier_df, on=\"sku\", how=\"left\")\n","\n","    # üßæ Upload result to Neon\n","    restock_df.to_sql(\"restock_recommendations\", engine, if_exists=\"replace\", index=False)\n","    print(\"‚úÖ Restock recommendations uploaded to Neon as 'restock_recommendations' table.\")\n","\n","# ‚úÖ Execute\n","generate_restock_report_from_neon()\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kbkGgVJFiYy","executionInfo":{"status":"ok","timestamp":1749329292258,"user_tz":240,"elapsed":2191,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"a10e013f-c66a-489c-845a-b703dccf938e"},"id":"-kbkGgVJFiYy","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Restock recommendations uploaded to Neon as 'restock_recommendations' table.\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"id":"V1sE6vQKGRkb","executionInfo":{"status":"error","timestamp":1749328862272,"user_tz":240,"elapsed":793,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"c263adb1-e70f-4a79-ab79-ec8ec110cfcd"},"id":"V1sE6vQKGRkb","execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'Quantity On Hand'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Quantity On Hand'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-e99e4faea8de>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_restock_report_from_neon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-173f4dd33265>\u001b[0m in \u001b[0;36mgenerate_restock_report_from_neon\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# üì• Read supplier data from supplier_master\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msupplier_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM supplier_master\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0msupplier_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Quantity On Hand\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupplier_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Quantity On Hand\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"coerce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# üìä Compute avg daily sales per SKU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Quantity On Hand'"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sqlalchemy import create_engine\n","\n","def generate_sourcing_recommendation_from_neon():\n","    # üîå Connect to Neon PostgreSQL\n","    db_uri = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","    engine = create_engine(db_uri)\n","\n","    # üì• Load trend data from Neon\n","    trend_df = pd.read_sql(\"SELECT * FROM trend_data\", engine)\n","\n","    # üì• Load supplier data from Neon\n","    supplier_df = pd.read_sql(\"SELECT * FROM supplier_master\", engine)\n","\n","    # üîó Merge by SKU\n","    merged_df = pd.merge(trend_df, supplier_df, on=\"sku\", how=\"inner\")\n","\n","    # üí∞ Calculate potential margin\n","    merged_df[\"potential_margin\"] = merged_df[\"avg_market_price\"] - merged_df[\"cost_price\"]\n","\n","    # üéØ Filter high margin & popularity\n","    filtered = merged_df[\n","        (merged_df[\"potential_margin\"] > 10) &\n","        (merged_df[\"popularity_score\"] >= 0.75)\n","    ].sort_values(by=[\"potential_margin\", \"popularity_score\"], ascending=False)\n","\n","    # üíæ Save recommendations back to Neon\n","    filtered.to_sql(\"sourcing_recommendations\", engine, if_exists=\"replace\", index=False)\n","\n","    print(\"‚úÖ Sourcing recommendations saved to Neon as 'sourcing_recommendations'\")\n","\n","# ‚úÖ Run the function\n","generate_sourcing_recommendation_from_neon()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTYYgazdIx5s","executionInfo":{"status":"ok","timestamp":1749330029454,"user_tz":240,"elapsed":805,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"f36309d9-8c17-47cc-dc57-51f3c2fb409d"},"id":"eTYYgazdIx5s","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Sourcing recommendations saved to Neon as 'sourcing_recommendations'\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sqlalchemy import create_engine\n","\n","def generate_final_inventory_actions_from_neon():\n","    # PostgreSQL connection string (Neon)\n","    conn_str = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","    engine = create_engine(conn_str)\n","\n","    # üì• Load restock and sourcing recommendations from Neon\n","    restock_df = pd.read_sql(\"SELECT * FROM restock_recommendations\", engine)\n","    sourcing_df = pd.read_sql(\"SELECT * FROM sourcing_recommendations\", engine)\n","\n","    # ‚úÖ Add action labels\n","    restock_df['action'] = 'Restock'\n","    sourcing_df['action'] = 'Source New'\n","\n","    # ‚úÖ Normalize columns for consistency\n","    restock_df = restock_df.rename(columns={\n","        'available_stock': 'stock_available',\n","        'avg_daily_sales': 'daily_sales_rate',\n","        'product_name': 'product_name'  # ensure it's named consistently\n","    })\n","\n","    sourcing_df = sourcing_df.rename(columns={\n","        'available_stock': 'stock_available',\n","        'product_name': 'product_name'\n","    })\n","\n","    # ‚úÖ Columns to align\n","    restock_cols = ['sku', 'product_name', 'stock_available', 'daily_sales_rate', 'days_coverage', 'action']\n","    sourcing_cols = ['sku', 'product_name', 'stock_available', 'popularity_score', 'potential_margin', 'action']\n","\n","    # Add missing columns with None to align both DataFrames\n","    for col in sourcing_cols:\n","        if col not in restock_df.columns:\n","            restock_df[col] = None\n","    for col in restock_cols:\n","        if col not in sourcing_df.columns:\n","            sourcing_df[col] = None\n","\n","    # ‚úÖ Combine both DataFrames\n","    combined_df = pd.concat([\n","        restock_df[restock_cols + list(set(sourcing_cols) - set(restock_cols))],\n","        sourcing_df[sourcing_cols + list(set(restock_cols) - set(sourcing_cols))]\n","    ], ignore_index=True)\n","\n","    # ‚úÖ Save combined actions to Neon\n","    combined_df.to_sql(\"final_inventory_actions\", engine, if_exists=\"replace\", index=False)\n","\n","    print(\"‚úÖ Final inventory actions saved to Neon as 'final_inventory_actions' table.\")\n","\n","# ‚ñ∂Ô∏è Run it\n","generate_final_inventory_actions_from_neon()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRsBbzZrMtWP","executionInfo":{"status":"ok","timestamp":1749330092448,"user_tz":240,"elapsed":1179,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"7d4b3830-9078-45cf-f294-cf3087cd85c4"},"id":"YRsBbzZrMtWP","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-39-789d64d0c89a>:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  combined_df = pd.concat([\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Final inventory actions saved to Neon as 'final_inventory_actions' table.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sqlalchemy import create_engine\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMRegressor\n","from sklearn.metrics import mean_absolute_error\n","import joblib\n","\n","# üì° Neon PostgreSQL connection\n","conn_str = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","engine = create_engine(conn_str)\n","\n","# üì• Load data from Neon\n","orders_df = pd.read_sql(\"SELECT sku, quantity, created_at FROM orders\", engine)\n","supplier_df = pd.read_sql(\"SELECT sku, available_stock, cost_price FROM supplier_master\", engine)\n","trend_df = pd.read_sql(\"SELECT sku, popularity_score, avg_market_price FROM trend_data\", engine)\n","\n","# üß† Merge supplier and trend data\n","product_info_df = pd.merge(supplier_df, trend_df, on=\"sku\", how=\"left\")\n","\n","# üßπ Clean and prepare orders data\n","orders_df['created_at'] = pd.to_datetime(orders_df['created_at'])\n","orders_df['day_of_week'] = orders_df['created_at'].dt.dayofweek\n","orders_df['day_of_month'] = orders_df['created_at'].dt.day\n","orders_df['quantity'] = pd.to_numeric(orders_df['quantity'], errors='coerce').fillna(0)\n","\n","# üìä Aggregate sales per SKU and date\n","agg_df = orders_df.groupby(['sku', 'created_at']).agg({'quantity': 'sum'}).reset_index()\n","\n","# üîÅ Merge with product info (supplier + trend)\n","merged_df = pd.merge(agg_df, product_info_df, on='sku', how='left')\n","\n","# üéØ Create 7-day rolling future target\n","merged_df = merged_df.sort_values(['sku', 'created_at'])\n","merged_df['target_sales'] = (\n","    merged_df.groupby('sku')['quantity']\n","    .shift(-1)\n","    .rolling(window=7)\n","    .sum()\n","    .reset_index(level=0, drop=True)\n",")\n","merged_df = merged_df.dropna(subset=['target_sales'])\n","\n","# üß™ Feature engineering\n","merged_df['day_of_week'] = merged_df['created_at'].dt.dayofweek\n","merged_df['day_of_month'] = merged_df['created_at'].dt.day\n","merged_df['available_stock'] = merged_df['available_stock'].fillna(0)\n","merged_df['popularity_score'] = merged_df['popularity_score'].fillna(0)\n","merged_df['avg_market_price'] = merged_df['avg_market_price'].fillna(merged_df['cost_price'] * 1.2)\n","\n","# ‚úÖ Define feature columns\n","features = ['available_stock', 'popularity_score', 'avg_market_price', 'day_of_week', 'day_of_month']\n","X = merged_df[features]\n","y = merged_df['target_sales']\n","\n","# üîÄ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","model = LGBMRegressor()\n","model.fit(X_train, y_train)\n","\n","# üìà Evaluate\n","y_pred = model.predict(X_test)\n","mae = mean_absolute_error(y_test, y_pred)\n","print(f\"‚úÖ MAE: {mae:.2f}\")\n","\n","# üßæ Prepare prediction DataFrame\n","sample_output = X_test.copy()\n","sample_output['actual_sales'] = y_test.values\n","sample_output['predicted_sales'] = y_pred\n","sample_output['sku'] = merged_df.loc[X_test.index, 'sku'].values\n","sample_output['created_at'] = merged_df.loc[X_test.index, 'created_at'].values\n","\n","# üì§ Save predictions to Neon PostgreSQL\n","sample_output.to_sql(\"lgbm_forecast_output\", engine, if_exists=\"replace\", index=False)\n","print(\"üìä Forecast saved to Neon table: 'lgbm_forecast_output'\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9BKJHJ2OFI8","executionInfo":{"status":"ok","timestamp":1749330607428,"user_tz":240,"elapsed":1019,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"5ca1507a-d2d7-49e2-83c3-64d5fd238dd6"},"id":"s9BKJHJ2OFI8","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 18\n","[LightGBM] [Info] Number of data points in the train set: 133, number of used features: 2\n","[LightGBM] [Info] Start training from score 7.218045\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","‚úÖ MAE: 0.23\n","üìä Forecast saved to Neon table: 'lgbm_forecast_output'\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from xgboost import XGBRegressor\n","from sqlalchemy import create_engine\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","\n","# PostgreSQL connection to Neon\n","conn_str = \"postgresql://neondb_owner:npg_4lvIfcDWR8gx@ep-little-cherry-a89bmbz9-pooler.eastus2.azure.neon.tech/neondb?sslmode=require\"\n","engine = create_engine(conn_str)\n","\n","# üì• Read orders data\n","orders_df = pd.read_sql(\"SELECT sku, quantity, created_at FROM orders\", engine)\n","orders_df[\"created_at\"] = pd.to_datetime(orders_df[\"created_at\"])\n","orders_df[\"date\"] = orders_df[\"created_at\"].dt.date\n","orders_df[\"quantity\"] = pd.to_numeric(orders_df[\"quantity\"], errors=\"coerce\").fillna(0)\n","\n","# üìä Aggregate sales per SKU per day\n","grouped = (\n","    orders_df.groupby([\"sku\", \"date\"])[\"quantity\"]\n","    .sum()\n","    .reset_index()\n","    .rename(columns={\"date\": \"ds\", \"quantity\": \"y\"})\n",")\n","\n","forecast_dfs = []\n","min_days_required = 5  # ‚úÖ lowered from 30\n","\n","for sku in grouped[\"sku\"].unique():\n","    sku_df = grouped[grouped[\"sku\"] == sku].copy()\n","    if len(sku_df) < min_days_required:\n","        continue\n","\n","    sku_df = sku_df.sort_values(\"ds\")\n","    sku_df[\"ds\"] = pd.to_datetime(sku_df[\"ds\"])\n","    sku_df[\"dayofweek\"] = sku_df[\"ds\"].dt.dayofweek\n","    sku_df[\"day\"] = sku_df[\"ds\"].dt.day\n","    sku_df[\"month\"] = sku_df[\"ds\"].dt.month\n","    sku_df[\"lag_1\"] = sku_df[\"y\"].shift(1)\n","    sku_df[\"lag_7\"] = sku_df[\"y\"].shift(7)\n","    sku_df = sku_df.dropna()\n","    if sku_df.shape[0] < 2:\n","        continue\n","    X = sku_df[[\"dayofweek\", \"day\", \"month\", \"lag_1\", \"lag_7\"]]\n","    y = sku_df[\"y\"]\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","\n","    model = XGBRegressor(n_estimators=100, learning_rate=0.1)\n","    model.fit(X_train, y_train)\n","\n","    # Predict for next 30 days\n","    future_dates = pd.date_range(sku_df[\"ds\"].max() + pd.Timedelta(days=1), periods=30)\n","    future_df = pd.DataFrame({\"ds\": future_dates})\n","    future_df[\"dayofweek\"] = future_df[\"ds\"].dt.dayofweek\n","    future_df[\"day\"] = future_df[\"ds\"].dt.day\n","    future_df[\"month\"] = future_df[\"ds\"].dt.month\n","    future_df[\"lag_1\"] = y.values[-1]\n","    future_df[\"lag_7\"] = y.values[-7] if len(y) >= 7 else y.values[-1]\n","\n","    future_df[\"yhat\"] = model.predict(future_df[[\"dayofweek\", \"day\", \"month\", \"lag_1\", \"lag_7\"]])\n","    future_df[\"sku\"] = sku\n","    forecast_dfs.append(future_df[[\"ds\", \"yhat\", \"sku\"]])\n","\n","# üì§ Save results\n","if forecast_dfs:\n","    result_df = pd.concat(forecast_dfs)\n","    result_df = result_df.rename(columns={\"ds\": \"forecast_date\", \"yhat\": \"predicted_quantity\"})\n","\n","    # Save to Neon PostgreSQL\n","    result_df.to_sql(\"sku_forecast_xgb\", engine, if_exists=\"replace\", index=False)\n","\n","    # Save to CSV for Tableau\n","    result_df.to_csv(\"xgb_forecast_output.csv\", index=False)\n","\n","    print(\"‚úÖ Forecasts saved to 'sku_forecast_xgb' table in Neon and CSV file generated.\")\n","else:\n","    print(\"‚ö†Ô∏è Still no SKUs with enough data for forecasting. Try with at least 5 records per SKU.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAva7EidPt3d","executionInfo":{"status":"ok","timestamp":1749331009011,"user_tz":240,"elapsed":753,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"16738d8e-7cd1-45d2-e329-43920e7295c7"},"id":"AAva7EidPt3d","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Forecasts saved to 'sku_forecast_xgb' table in Neon and CSV file generated.\n"]}]},{"cell_type":"code","source":["sku_counts = grouped['sku'].value_counts()\n","print(sku_counts.head(20))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qmJUEIjP28T","executionInfo":{"status":"ok","timestamp":1749330916802,"user_tz":240,"elapsed":18,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"94bf9463-543c-484f-e9ab-80200cf17533"},"id":"-qmJUEIjP28T","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sku\n","C-20714156893      9\n","C-3614272050341    9\n","C-088300101306     8\n","C-3605533286555    6\n","C-8011003861910    6\n","C-8011003839100    5\n","C-20714080310      5\n","C-8018365071162    4\n","C-88300603404      4\n","C-3614272872387    4\n","C-8011003809196    4\n","D-3614272648425    4\n","C-88300106509      4\n","D-3386460121514    4\n","D-3348901250146    3\n","D-3605972321794    3\n","D-888066117463     3\n","C-8011003823529    3\n","C-608940553893     3\n","C-3614226905666    3\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["# Look up to four levels deep under /content for your notebook\n","!find /content -maxdepth 4 -type f -name \"databricks_inventory_forecast_xgboost.ipynb\" -print"],"metadata":{"id":"y9JR4sX0wY6m","executionInfo":{"status":"ok","timestamp":1751471193903,"user_tz":240,"elapsed":48,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}}},"id":"y9JR4sX0wY6m","execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"6Ptqw7zP0ja-"},"id":"6Ptqw7zP0ja-"},{"cell_type":"code","source":["!cp ../databricks_inventory_forecast_xgboost.ipynb \\\n","     notebooks/databricks_inventory_forecast_xgboost.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bm79y0hj0KB3","executionInfo":{"status":"ok","timestamp":1751471161343,"user_tz":240,"elapsed":85,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"f248db8d-40fc-4468-bbcf-d40b579da2f8"},"id":"Bm79y0hj0KB3","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '../databricks_inventory_forecast_xgboost.ipynb': No such file or directory\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPrlO97-00wx","executionInfo":{"status":"ok","timestamp":1751471451234,"user_tz":240,"elapsed":19369,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"9e7721fb-9a8c-45fc-dd82-a56e9bcede64"},"id":"yPrlO97-00wx","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!find /content/drive -name \"*.ipynb\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZsDVaIpb1ZlO","executionInfo":{"status":"ok","timestamp":1751471830035,"user_tz":240,"elapsed":362491,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"bfb86d8c-5398-4bec-82cd-6bc62c3aee5f"},"id":"ZsDVaIpb1ZlO","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Untitled0.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled.ipynb\n","/content/drive/MyDrive/Colab Notebooks/RedditSentimentAnalysis (1).ipynb\n","/content/drive/MyDrive/Colab Notebooks/RedditSentimentAnalysis.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled1.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled2.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled3.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled4.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled5.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled6.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled7.ipynb\n","/content/drive/MyDrive/Colab Notebooks/SalesPrediction.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Copy of databricks_inventory_forecast_xgboost.ipynb\n","/content/drive/MyDrive/Colab Notebooks/databricks_inventory_forecast_xgboost.ipynb\n","^C\n"]}]},{"cell_type":"code","source":["from urllib.parse import quote\n","import getpass\n","\n","token =\"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\"\n","username = 'canikhil12'\n","repo  = \"Sales_forecasting\"\n","# Create authenticated HTTPS URL with token\n","url = f\"https://{token}@github.com/{username}/{repo}.git\"\n","print (remote_url)\n","# Set the remote to use the new URL\n","!git remote add origin {remote_url}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UprLVqlv5fLU","executionInfo":{"status":"ok","timestamp":1751473901530,"user_tz":240,"elapsed":104,"user":{"displayName":"Ch Akhil","userId":"13667991779245176939"}},"outputId":"1c057bf4-9798-4e4b-d95e-6836b33ddffa"},"id":"UprLVqlv5fLU","execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["https://Akhil%40134@github.com/canikhil12/Sales_forecasting.git\n"]}]}],"metadata":{"language":"python","colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}